# path to the task data directory
data_dir: "data"
preprocess_data: False

goal: null
eval: null

log_dir: logs
workspace_dir: workspaces

# whether to copy the data to the workspace directory (otherwise it will be symlinked)
# copying is recommended to prevent the agent from accidentally modifying the original data
copy_data: True

exp_name: run # a random experiment name will be generated if not provided

# settings for code execution
exec:
  timeout: 3600
  agent_file_name: runfile.py
  format_tb_ipython: False

generate_report: True
# LLM settings for final report from journal - Using GPT-4o-mini for consistency
report:
  model: gpt-4o-mini  # Use GPT-4o-mini for report consistency
  temp: 0.7  # Reduced temperature for more focused output

experiment:
  num_syn_datasets: 1

debug:
  stage4: False

# agent hyperparams - COST-OPTIMIZED CONFIG
agent:
  type: parallel
  num_workers: 2  # Reduced from 4 to save on parallel API calls
  stages:
    stage1_max_iters: 8   # Reduced from 20
    stage2_max_iters: 6   # Reduced from 12  
    stage3_max_iters: 6   # Reduced from 12
    stage4_max_iters: 8   # Reduced from 18
  # how many improvement iterations to run
  steps: 3 # Reduced from 5 to limit total steps
  # whether to instruct the agent to use CV (set to 1 to disable)
  k_fold_validation: 1
  multi_seed_eval:
    num_seeds: 2 # Reduced from 3 to save on evaluation runs
  # whether to instruct the agent to generate a prediction function
  expose_prediction: False
  # whether to provide the agent with a preview of the data
  data_preview: False

  # LLM settings for coding - using GPT-4o-mini (reliable function calling)
  code:
    model: gpt-4o-mini  # Use GPT-4o-mini for reliable function calling
    temp: 0.7  # Reduced from 1.0 for more focused responses
    max_tokens: 8000  # Reduced from 12000

  # LLM settings for evaluating program output / tracebacks  
  feedback:
    model: gpt-4o-mini  # Use GPT-4o-mini for reliable function calling
    temp: 0.5
    max_tokens: 4096  # Reduced from 8192

  vlm_feedback:
    model: gpt-4o-mini  # Use GPT-4o-mini for reliable function calling
    temp: 0.5
    max_tokens: 4096  # Set limit instead of null

  search:
    max_debug_depth: 2  # Reduced from 3
    debug_prob: 0.3     # Reduced from 0.5
    num_drafts: 2       # Reduced from 3
